# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2018, Qiskit Development Team
# This file is distributed under the same license as the Qiskit package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2019.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Qiskit \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2019-01-31 13:06-0500\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.6.0\n"

#: ../../aqua/optimizers.rst:5
msgid "Optimizers"
msgstr ""

#: ../../aqua/optimizers.rst:7
msgid ""
"Aqua  contains a variety of classical optimizers for use by quantum "
"variational algorithms, such as :ref:`vqe`. Logically, these optimizers "
"can be divided into two categories:"
msgstr ""

#: ../../aqua/optimizers.rst:11
msgid ""
":ref:`Local Optimizers`: Given an optimization problem, a *local "
"optimizer* is a function that attempts to find an optimal value within "
"the neighboring set of a candidate solution."
msgstr ""

#: ../../aqua/optimizers.rst:14
msgid ""
":ref:`Global Optimizers`: Given an optimization problem, a *global "
"optimizer* is a function that attempts to find an optimal value among all"
" possible solutions."
msgstr ""

msgid "Extending the Optimizer Library"
msgstr ""

#: ../../aqua/optimizers.rst:20
msgid ""
"Consistent with its unique  design, Aqua has a modular and extensible "
"architecture. Algorithms and their supporting objects, such as optimizers"
" for quantum variational algorithms, are pluggable modules in Aqua. New "
"optimizers for quantum variational algorithms are typically installed in "
"the ``qiskit_aqua/utils/optimizers`` folder and derive from the "
"``Optimizer`` class.  Aqua also allows for :ref:`aqua-dynamically-"
"discovered-components`: new optimizers can register themselves as Aqua "
"extensions and be dynamically discovered at run time independent of their"
" location in the file system. This is done in order to encourage "
"researchers and developers interested in :ref:`aqua-extending` to extend "
"the Aqua framework with their novel research contributions."
msgstr ""

#: ../../aqua/optimizers.rst:34
msgid ""
"`Section :ref:`aqua-extending` provides more details on how to extend "
"Aqua with new components."
msgstr ""

#: ../../aqua/optimizers.rst:41
msgid "Local Optimizers"
msgstr ""

#: ../../aqua/optimizers.rst:43
msgid ""
"This section presents the classical local optimizers made available in "
"Aqua. These optimizers are meant to be used in conjunction with quantum "
"variational algorithms:"
msgstr ""

#: ../../aqua/optimizers.rst:47
msgid ":ref:`Conjugate Gradient (CG) Method`"
msgstr ""

#: ../../aqua/optimizers.rst:48
msgid ":ref:`Constrained Optimization BY Linear Approximation (COBYLA)`"
msgstr ""

#: ../../aqua/optimizers.rst:49
msgid ":ref:`Limited-memory Broyden-Fletcher-Goldfarb-Shanno Bound (L-BFGS-B)`"
msgstr ""

#: ../../aqua/optimizers.rst:50
msgid ":ref:`Nelder-Mead`"
msgstr ""

#: ../../aqua/optimizers.rst:51
msgid ":ref:`Parallel Broyden-Fletcher-Goldfarb-Shann (P-BFGS)`"
msgstr ""

#: ../../aqua/optimizers.rst:52
msgid ":ref:`Powell`"
msgstr ""

#: ../../aqua/optimizers.rst:53
msgid ":ref:`Sequential Least SQuares Programming (SLSQP)`"
msgstr ""

#: ../../aqua/optimizers.rst:54
msgid ":ref:`Simultaneous Perturbation Stochastic Approximation (SPSA)`"
msgstr ""

#: ../../aqua/optimizers.rst:55
msgid ":ref:`Truncated Newton (TNC)`"
msgstr ""

#: ../../aqua/optimizers.rst:57
msgid ""
"Except for :ref:`Parallel Broyden-Fletcher-Goldfarb-Shann (P-BFGS)`, all "
"these optimizers are directly based on the ``scipy.optimize.minimize`` "
"optimization function in the `SciPy "
"<https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html>`__"
" Python library. They all have a common pattern for parameters. "
"Specifically, the ``tol`` parameter, whose value must be a ``float`` "
"indicating *tolerance for termination*, is from the "
"``scipy.optimize.minimize``  method itself, while the remaining "
"parameters are from the `options dictionary "
"<https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.show_options.html>`__,"
" which may be referred to for further information."
msgstr ""

#: ../../aqua/optimizers.rst:68
msgid ""
"Aqua comes with a large collection of adaptive algorithms, such as the "
"`Variational Quantum Eigensolver (VQE) algorithm "
"<https://www.nature.com/articles/ncomms5213>`__, `Quantum Approximate "
"Optimization Algorithm (QAOA) <https://arxiv.org/abs/1411.4028>`__, the "
"`Quantum Support Vector Machine (SVM) Variational Algorithm "
"<https://arxiv.org/abs/1804.11326>`__ for AI. All these algorithms "
"interleave quantum and classical computations, making use of classical "
"optimizers. Aqua includes nine local and five global optimizers to choose"
" from. By profiling the execution of the adaptive algorithms, we have "
"detected that a large portion of the execution time is taken by the "
"optimization phase, which runs classically. Among the most widely used "
"optimizers are the *gradient-based* ones; these optimizers attempt to "
"compute the absolute minimum (or maximum) of a function :math:`f` through"
" its gradient."
msgstr ""

#: ../../aqua/optimizers.rst:83
msgid ""
"Five local optimizers among those integrated into Aqua are gradient-"
"based: the four local optimizers *Limited-memory Broyden-Fletcher-"
"Goldfarb-Shanno Bound (L-BFGS-B)*, *Sequential Least SQuares Programming "
"(SLSQP)*, *Conjugate Gradient (CG)*, and *Truncated Newton (TNC)* from "
"`SciPy "
"<https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html>`__,"
" as well as `Simultaneous Perturbation Stochastic Approximation (SPSA) "
"<https://www.jhuapl.edu/SPSA/>`__. Aqua contains a methodology that "
"parallelizes the classical computation of the partial derivatives in the "
"gradient-based local optimizers listed above. This parallelization takes "
"place *transparently*, in the sense that Aqua intercepts the computation "
"of the partial derivatives and parallelizes it without making any change "
"to the actual source code of the optimizers."
msgstr ""

#: ../../aqua/optimizers.rst:97
#, python-format
msgid ""
"In order to activate the parallelization mechanism for an adaptive "
"algorithm included in Aqua, it is sufficient to construct it with "
"parameter ``batch_mode`` set to ``True``. Our experiments have proven "
"empirically that parallelizing the process of a gradient-based local "
"optimizer achieves a 30% speedup in the execution time of an adaptive "
"algorithms on a simulator."
msgstr ""

#: ../../aqua/optimizers.rst:108
msgid "Conjugate Gradient (CG) Method"
msgstr ""

#: ../../aqua/optimizers.rst:109
msgid ""
"CG is an algorithm for the numerical solution of systems of linear "
"equations whose matrices are symmetric and positive-definite. It is an "
"*iterative algorithm* in that it uses an initial guess to generate a "
"sequence of improving approximate solutions for a problem, in which each "
"approximation is derived from the previous ones.  It is often used to "
"solve unconstrained optimization problems, such as energy minimization."
msgstr ""

#: ../../aqua/optimizers.rst:113 ../../aqua/optimizers.rst:228
#: ../../aqua/optimizers.rst:288 ../../aqua/optimizers.rst:397
#: ../../aqua/optimizers.rst:462 ../../aqua/optimizers.rst:619
msgid "The following parameters are supported:"
msgstr ""

#: ../../aqua/optimizers.rst:115 ../../aqua/optimizers.rst:172
msgid "The maximum number of iterations to perform:"
msgstr ""

#: ../../aqua/optimizers.rst:121
msgid "This parameters takes a positive ``int`` value.  The default is ``20``."
msgstr ""

#: ../../aqua/optimizers.rst:123 ../../aqua/optimizers.rst:180
#: ../../aqua/optimizers.rst:629
msgid "A Boolean value indicating whether or not to print convergence messages:"
msgstr ""

#: ../../aqua/optimizers.rst:129 ../../aqua/optimizers.rst:186
#: ../../aqua/optimizers.rst:635
msgid "The default value is ``False``."
msgstr ""

#: ../../aqua/optimizers.rst:131
msgid ""
"A tolerance value that must be greater than the gradient norm before "
"successful termination."
msgstr ""

#: ../../aqua/optimizers.rst:137
msgid "The default value is ``1e-05``."
msgstr ""

#: ../../aqua/optimizers.rst:140 ../../aqua/optimizers.rst:196
#: ../../aqua/optimizers.rst:324 ../../aqua/optimizers.rst:433
#: ../../aqua/optimizers.rst:488 ../../aqua/optimizers.rst:670
msgid "The tolerance for termination:"
msgstr ""

#: ../../aqua/optimizers.rst:146
msgid ""
"This parameter is optional.  If specified, the value of this parameter "
"must be a ``float`` value, otherwise, it is set to ``None``.  The default"
" is ``None``."
msgstr ""

#: ../../aqua/optimizers.rst:149 ../../aqua/optimizers.rst:497
#: ../../aqua/optimizers.rst:679
msgid "Step size used for numerical approximation of the Jacobian."
msgstr ""

#: ../../aqua/optimizers.rst:155 ../../aqua/optimizers.rst:685
msgid "The default value is ``1.4901161193847656e-08``."
msgstr ""

msgid "Declarative Name"
msgstr ""

#: ../../aqua/optimizers.rst:159
msgid ""
"When referring to CG declaratively inside Aqua, its code ``name``, by "
"which Aqua dynamically discovers and loads it, is ``CG``."
msgstr ""

#: ../../aqua/optimizers.rst:166
msgid "Constrained Optimization BY Linear Approximation (COBYLA)"
msgstr ""

#: ../../aqua/optimizers.rst:168
msgid ""
"COBYLA is a numerical optimization method for constrained problems where "
"the derivative of the objective function is not known. COBYLA supports "
"the following parameters:"
msgstr ""

#: ../../aqua/optimizers.rst:178 ../../aqua/optimizers.rst:236
#: ../../aqua/optimizers.rst:305
msgid "A positive ``int`` value is expected.  The default is ``1000``."
msgstr ""

#: ../../aqua/optimizers.rst:188
msgid "Reasonable initial changes to the variable:"
msgstr ""

#: ../../aqua/optimizers.rst:194
msgid "The default value is ``1.0``."
msgstr ""

#: ../../aqua/optimizers.rst:202
msgid ""
"This parameter is optional.  If specified, the value of this parameter "
"must be of type ``float``, otherwise, it is set to ``None``. The default "
"is ``None``."
msgstr ""

#: ../../aqua/optimizers.rst:207
msgid ""
"When referring to COBYLA declaratively inside Aqua, its code ``name``, by"
" which Aqua dynamically discovers and loads it, is ``COBYLA``."
msgstr ""

#: ../../aqua/optimizers.rst:214
msgid "Limited-memory Broyden-Fletcher-Goldfarb-Shanno Bound (L-BFGS-B)"
msgstr ""

#: ../../aqua/optimizers.rst:216
msgid ""
"The target goal of L-BFGS-B is to minimize the value of a differentiable "
"scalar function :math:`f`. This optimizer is a *quasi-Newton method*, "
"meaning that, in contrast to *Newtons's method*, it does not require "
":math:`f`'s *Hessian* (the matrix of :math:`f`'s second derivatives) when"
" attempting to compute :math:`f`'s minimum value. Like BFGS, L-BFGS is an"
" iterative method for solving unconstrained, non-linear optimization "
"problems, but approximates BFGS using a limited amount of computer "
"memory. L-BFGS starts with an initial estimate of the optimal value, and "
"proceeds iteratively to refine that estimate with a sequence of better "
"estimates. The derivatives of :math:`f` are used to identify the "
"direction of steepest descent, and also to form an estimate of the "
"Hessian matrix (second derivative) of :math:`f`. L-BFGS-B extends L-BFGS "
"to handle simple, per-variable bound constraints."
msgstr ""

#: ../../aqua/optimizers.rst:230
msgid "The maximum number of function evaluations:"
msgstr ""

#: ../../aqua/optimizers.rst:238 ../../aqua/optimizers.rst:290
#: ../../aqua/optimizers.rst:399 ../../aqua/optimizers.rst:464
#: ../../aqua/optimizers.rst:621
msgid "The maximum number of iterations:"
msgstr ""

#: ../../aqua/optimizers.rst:244
msgid "A positive ``int`` value is expected.  The default is ``10``."
msgstr ""

#: ../../aqua/optimizers.rst:246
msgid ""
"An ``int`` value controlling the frequency of the printed output showing "
"the  optimizer's operations:"
msgstr ""

#: ../../aqua/optimizers.rst:253
msgid "The default is ``-1``."
msgstr ""

#: ../../aqua/optimizers.rst:255
msgid "Step size used if numerically calculating the gradient."
msgstr ""

#: ../../aqua/optimizers.rst:261 ../../aqua/optimizers.rst:503
msgid "The default value is ``1e-08``."
msgstr ""

#: ../../aqua/optimizers.rst:264
msgid ""
"Further detailed information on ``factr`` and ``iprint`` may be found at "
"`scipy.optimize.fmin_l_bfgs_b "
"<https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html>`__."
msgstr ""

#: ../../aqua/optimizers.rst:269
msgid ""
"When referring to L-BFGS-B declaratively inside Aqua, its code ``name``, "
"by which Aqua dynamically discovers and loads it, is ``L_BFGS_B``."
msgstr ""

#: ../../aqua/optimizers.rst:276
msgid "Nelder-Mead"
msgstr ""

#: ../../aqua/optimizers.rst:278
msgid ""
"The Nelder-Mead algorithm performs unnconstrained optimization; it "
"ignores bounds or constraints.  It is used to find the minimum or maximum"
" of an objective function in a multidimensional space.  It is based on "
"the Simplex algorithm. Nelder-Mead is robust in many applications, "
"especially when the first and second derivatives of the objective "
"function are not known. However, if the numerical computation of the "
"derivatives can be trusted to be accurate, other algorithms using the "
"first and/or second derivatives information might be preferred to Nelder-"
"Mead for their better performance in the general case, especially in "
"consideration of the fact that the Nelderâ€“Mead technique is a heuristic "
"search method that can converge to non-stationary points."
msgstr ""

#: ../../aqua/optimizers.rst:296 ../../aqua/optimizers.rst:405
msgid ""
"This parameter is optional.  If specified, the value of this parameter "
"must be a positive ``int``, otherwise, it is  ``None``. The default is "
"``None``."
msgstr ""

#: ../../aqua/optimizers.rst:299 ../../aqua/optimizers.rst:408
msgid "The maximum number of functional evaluations to perform:"
msgstr ""

#: ../../aqua/optimizers.rst:307 ../../aqua/optimizers.rst:416
#: ../../aqua/optimizers.rst:472
msgid "A ``bool`` value indicating whether or not to print convergence messages:"
msgstr ""

#: ../../aqua/optimizers.rst:313 ../../aqua/optimizers.rst:337
#: ../../aqua/optimizers.rst:422 ../../aqua/optimizers.rst:478
msgid "The default is ``False``."
msgstr ""

#: ../../aqua/optimizers.rst:315 ../../aqua/optimizers.rst:424
msgid ""
"A tolerance parameter indicating the absolute error in ``xopt`` between "
"iterations that will be considered acceptable for convergence."
msgstr ""

#: ../../aqua/optimizers.rst:322 ../../aqua/optimizers.rst:431
msgid "The default value is ``0.0001``."
msgstr ""

#: ../../aqua/optimizers.rst:330 ../../aqua/optimizers.rst:439
msgid ""
"This parameter is optional.  If specified, the value of this parameter "
"must be of type ``float``, otherwise, it is  ``None``. The default is "
"``None``."
msgstr ""

#: ../../aqua/optimizers.rst:339
msgid "If true will adapt algorithm to dimensionality of problem."
msgstr ""

#: ../../aqua/optimizers.rst:343
msgid ""
"When referring to Nelder-Mead declaratively inside Aqua, its code "
"``name``, by which Aqua dynamically discovers and loads it, is "
"``NELDER_MEAD``."
msgstr ""

#: ../../aqua/optimizers.rst:350
msgid "Parallel Broyden-Fletcher-Goldfarb-Shann (P-BFGS)"
msgstr ""

#: ../../aqua/optimizers.rst:352
msgid ""
"P-BFGS is a parallellized version of  `L-BFGS-B <#limited-memory-broyden-"
"fletcher-goldfarb-shanno-bound-l-bfgs-b>`__, with which it shares the "
"same parameters. P-BFGS can be useful when the target hardware is a "
"quantum simulator running on a classical machine. This allows the "
"multiple processes to use simulation to potentially reach a minimum "
"faster. The parallelization may help the optimizer avoid getting stuck at"
" local optima.  In addition to the parameters of L-BFGS-B, P-BFGS "
"supports an following parameter --- the maximum number of processes "
"spawned by P-BFGS:"
msgstr ""

#: ../../aqua/optimizers.rst:364
msgid ""
"By default, P-BFGS runs one optimization in the current process and "
"spawns additional processes up to the number of processor cores. An "
"``int`` value may be specified to limit the total number of processes (or"
" cores) used.  This parameter is optional.  If specified, the value of "
"this parameter must be a positive ``int``, otherwise, it is ``None``.  "
"The default is ``None``."
msgstr ""

#: ../../aqua/optimizers.rst:372
msgid ""
"The parallel processes do not currently work for this optimizer on the "
"Microsoft Windows platform. There, P-BFGS will just run the one "
"optimization in the main process, without spawning new processes. "
"Therefore, the resulting behavior will be the same as the L-BFGS-B "
"optimizer."
msgstr ""

#: ../../aqua/optimizers.rst:380
msgid ""
"When referring to P-BFGS declaratively inside Aqua, its code ``name``, by"
" which Aqua dynamically discovers and loads it, is ``P_BFGS``."
msgstr ""

#: ../../aqua/optimizers.rst:388
msgid "Powell"
msgstr ""

#: ../../aqua/optimizers.rst:390
msgid ""
"The Powell algorithm performs unconstrained optimization; it ignores "
"bounds or constraints. Powell is a *conjugate direction method*: it "
"performs sequential one-dimensional minimization along each directional "
"vector, which is updated at each iteration of the main minimization loop."
" The function being minimized need not be differentiable, and no "
"derivatives are taken."
msgstr ""

#: ../../aqua/optimizers.rst:414 ../../aqua/optimizers.rst:544
msgid "A positive ``int`` value is expected.  The default value is ``1000``."
msgstr ""

#: ../../aqua/optimizers.rst:444
msgid ""
"When referring to Powell declaratively inside Aqua, its code ``name``, by"
" which Aqua dynamically discovers and loads it, is ``POWELL``."
msgstr ""

#: ../../aqua/optimizers.rst:451
msgid "Sequential Least SQuares Programming (SLSQP)"
msgstr ""

#: ../../aqua/optimizers.rst:453
msgid ""
"SLSQP minimizes a function of several variables with any combination of "
"bounds, equality and inequality constraints. The method wraps the SLSQP "
"Optimization subroutine originally implemented by Dieter Kraft. SLSQP is "
"ideal for  mathematical problems for which the objective function and the"
" constraints are twice continuously differentiable. Note that the wrapper"
" handles infinite values in bounds by converting them into large floating"
" values."
msgstr ""

#: ../../aqua/optimizers.rst:470 ../../aqua/optimizers.rst:627
msgid "A positive ``int`` value is expected.  The default is ``100``."
msgstr ""

#: ../../aqua/optimizers.rst:480
msgid ""
"A tolerance value indicating precision goal for the value of the "
"objective function in the stopping criterion."
msgstr ""

#: ../../aqua/optimizers.rst:486
msgid "A ``float`` value is expected.  The default value is ``1e-06``."
msgstr ""

#: ../../aqua/optimizers.rst:494
msgid ""
"This parameter is optional.  If specified, the value of this parameter "
"must be a ``float``, otherwise, it is  ``None``. The default is ``None``."
msgstr ""

#: ../../aqua/optimizers.rst:507
msgid ""
"When referring to SLSQP declaratively inside Aqua, its code ``name``, by "
"which Aqua dynamically discovers and loads it, is ``SLSQP``."
msgstr ""

#: ../../aqua/optimizers.rst:514
msgid "Simultaneous Perturbation Stochastic Approximation (SPSA)"
msgstr ""

#: ../../aqua/optimizers.rst:516
msgid ""
"SPSA is an algorithmic method for optimizing systems with multiple "
"unknown parameters. As an optimization method, it is appropriately suited"
" to large-scale population models, adaptive modeling, and simulation "
"optimization."
msgstr ""

#: ../../aqua/optimizers.rst:520
msgid ""
"Many examples are presented at the `SPSA Web site "
"<http://www.jhuapl.edu/SPSA>`__."
msgstr ""

#: ../../aqua/optimizers.rst:522
msgid ""
"SPSA is a descent method capable of finding global minima, sharing this "
"property with other methods as simulated annealing. Its main feature is "
"the gradient approximation, which requires only two measurements of the "
"objective function, regardless of the dimension of the optimization "
"problem."
msgstr ""

#: ../../aqua/optimizers.rst:529
msgid ""
"SPSA can be used in the presence of noise, and it is therefore indicated "
"in situations involving measurement uncertainty on a quantum computation "
"when finding a minimum. If you are executing a variational algorithm "
"using a Quantum ASseMbly Language (QASM) simulator or a real device, SPSA"
" would be the most recommended choice among the optimizers provided here."
msgstr ""

#: ../../aqua/optimizers.rst:534
msgid ""
"The optimization process includes a calibration phase, which requires "
"additional functional evaluations.  Overall, the following parameters are"
" supported:"
msgstr ""

#: ../../aqua/optimizers.rst:537
msgid ""
"Maximum number of trial steps to be taken for the optimization. There are"
" two function evaluations per trial:"
msgstr ""

#: ../../aqua/optimizers.rst:546
msgid ""
"An ``int`` value determining how often optimization outcomes should be "
"stored during execution:"
msgstr ""

#: ../../aqua/optimizers.rst:552
msgid ""
"A positive ``int`` value is expected. SPSA will store optimization "
"outcomes every ``save_steps`` trial steps.  The default value is ``1``."
msgstr ""

#: ../../aqua/optimizers.rst:555
msgid ""
"The number of last updates of the variables to average on for the final "
"objective function:"
msgstr ""

#: ../../aqua/optimizers.rst:562
msgid "A positive ``int`` value is expected.  The default value is ``1``."
msgstr ""

#: ../../aqua/optimizers.rst:564
msgid "Control parameters for SPSA:"
msgstr ""

#: ../../aqua/optimizers.rst:574
msgid ""
"These are the SPSA control parameters, consisting of 5 ``float`` values, "
"and are used as described below."
msgstr ""

#: ../../aqua/optimizers.rst:576
msgid ""
"SPSA updates the parameters (``theta``) for the objective function "
"(``J``) through the following equation at iteration ``k``:"
msgstr ""

#: ../../aqua/optimizers.rst:588
msgid ""
"``J(theta)`` is the  objective value of ``theta``. ``c0``, ``c1``, "
"``c2``, ``c3`` and ``c4`` are the five control parameters. By default, "
"``c0`` is calibrated through a few evaluations on the objective function "
"with the initial ``theta``. ``c1``, ``c2``, ``c3`` and ``c4`` are set as "
"``0.1``, ``0.602``, ``0.101``, ``0.0``, respectively."
msgstr ""

#: ../../aqua/optimizers.rst:593
msgid "Calibration step for SPSA."
msgstr ""

#: ../../aqua/optimizers.rst:599
msgid ""
"The default value is ``False``. When calibration is done, i.e. when "
"``skip_calibration`` is ``False`` (by default) the control parameter "
"``c0`` as supplied is adjusted by the calibration step before "
"optimization. If ``skip_calibration`` is ``True`` then the calibration "
"step, which occurs ahead of optimization, is skipped and ``c0`` will be "
"used unaltered."
msgstr ""

#: ../../aqua/optimizers.rst:605
msgid ""
"When referring to SPSA declaratively inside Aqua, its code ``name``, by "
"which Aqua dynamically discovers and loads it, is ``SPSA``."
msgstr ""

#: ../../aqua/optimizers.rst:612
msgid "Truncated Newton (TNC)"
msgstr ""

#: ../../aqua/optimizers.rst:613
msgid ""
"TNC uses a truncated Newton algorithm to minimize a function with "
"variables subject to bounds. This algorithm uses gradient information; it"
" is also called Newton Conjugate-Gradient. It differs from the "
":ref:`Conjugate Gradient (CG) Method` method as it wraps a C "
"implementation and allows each variable to be given upper and lower "
"bounds."
msgstr ""

#: ../../aqua/optimizers.rst:637
msgid "Relative precision for finite difference calculations:"
msgstr ""

#: ../../aqua/optimizers.rst:643
msgid "The default value is ``0.0``."
msgstr ""

#: ../../aqua/optimizers.rst:645
msgid ""
"A tolerance value indicating the precision goal for the value of the "
"objective function ``f`` in the stopping criterion."
msgstr ""

#: ../../aqua/optimizers.rst:651 ../../aqua/optimizers.rst:659
#: ../../aqua/optimizers.rst:668
msgid "The default value is ``-1``."
msgstr ""

#: ../../aqua/optimizers.rst:653
msgid ""
"A tolerance value indicating precision goal for the value of ``x`` in the"
" stopping criterion, after applying ``x`` scaling factors."
msgstr ""

#: ../../aqua/optimizers.rst:661
msgid ""
"A tolerance value indicating precision goal for the value of the "
"projected gradient ``g`` in the stopping criterion, after applying ``x`` "
"scaling factors."
msgstr ""

#: ../../aqua/optimizers.rst:676
msgid ""
"This parameter is optional.  If specified, the value of this parameter "
"must be a ``float``, otherwise, it is  ``None``. The default is ``None``"
msgstr ""

#: ../../aqua/optimizers.rst:689
msgid ""
"When referring to TNC declaratively inside Aqua, its code ``name``, by "
"which Aqua dynamically discovers and loads it, is ``TNC``."
msgstr ""

#: ../../aqua/optimizers.rst:696
msgid "Global Optimizers"
msgstr ""

#: ../../aqua/optimizers.rst:697
msgid ""
"Aqua supports a number of classical global optimizers, all based on the "
"open-source `NonLinear optimization (NLopt) library "
"<https://nlopt.readthedocs.io>`__. Each of these optimizers uses the "
"corresponding named optimizer from NLopt. This package has native code "
"implementations and must be installed locally for these global optimizers"
" to be accessible by Aqua. Wrapper code allowing Aqua to interface these "
"optimizers is installed in the ``nlopt`` subfolder of the ``optimizers`` "
"folder."
msgstr ""

msgid "Installation of NLopt"
msgstr ""

#: ../../aqua/optimizers.rst:707
msgid ""
"The `NLopt download and installation instructions "
"<https://nlopt.readthedocs.io/en/latest/#download-and-installation>`__ "
"describe how to install NLopt."
msgstr ""

#: ../../aqua/optimizers.rst:710
msgid ""
"If you running Aqua on Windows, then you might want to refer to the "
"specific `instructions for NLopt on Windows "
"<https://nlopt.readthedocs.io/en/latest/NLopt_on_Windows/>`__."
msgstr ""

#: ../../aqua/optimizers.rst:713
msgid ""
"If you are running Aqua on a Unix-like system, first ensure that your "
"environment is set to the Python executable for which the qiskit_aqua "
"package is installed and running. Now, having downloaded and unpacked the"
" NLopt archive file (for example, ``nlopt-2.4.2.tar.gz`` for version "
"2.4.2), enter the following commands:"
msgstr ""

#: ../../aqua/optimizers.rst:724
msgid ""
"The above makes and installs the shared libraries and Python interface in"
" `/usr/local`. To have these be used by Aqua, the following commands can "
"be entered to augment the dynamic library load path and python path "
"respectively, assuming that you choose to leave these entities where they"
" were built and installed as per above commands and that you are running "
"Python 3.6:"
msgstr ""

#: ../../aqua/optimizers.rst:734
msgid ""
"The two ``export`` commands above can be pasted into the "
"``.bash_profile`` file in the user's home directory for automatic "
"execution.  Now you can run Aqua and these optimizers should be available"
" for you to use."
msgstr ""

msgid "The ``max_evals`` Parameter"
msgstr ""

#: ../../aqua/optimizers.rst:739
msgid ""
"All the NLopt optimizers are supported by a common interface, allowing "
"the optimizers to share the same common parameters. For quantum "
"variational algorithms, it is necessary to assign a value to the "
"following parameter:"
msgstr ""

#: ../../aqua/optimizers.rst:748
msgid ""
"This parameter takes a positive ``int`` as its value, indicating the "
"maximum object function evaluation.  The default value is ``1000``."
msgstr ""

#: ../../aqua/optimizers.rst:751
msgid "Currently, Aqua supplies the following global optimizers from NLOpt:"
msgstr ""

#: ../../aqua/optimizers.rst:753
msgid ":ref:`Controller Random Search (CRS) with Local Mutation`"
msgstr ""

#: ../../aqua/optimizers.rst:754
msgid ":ref:`DIviding RECTangles algorithm - Locally based (DIRECT-L)`"
msgstr ""

#: ../../aqua/optimizers.rst:755
msgid ""
":ref:`DIviding RECTangles algorithm - Locally based - RANDomized "
"(DIRECT-L-RAND)`"
msgstr ""

#: ../../aqua/optimizers.rst:756
msgid ":ref:`Evolutionary Strategy algorithm with CaucHy distribution (ESCH)`"
msgstr ""

#: ../../aqua/optimizers.rst:757
msgid ":ref:`Improved Stochastic Ranking Evolution Strategy (ISRES)`"
msgstr ""

#: ../../aqua/optimizers.rst:763
msgid "Controller Random Search (CRS) with Local Mutation"
msgstr ""

#: ../../aqua/optimizers.rst:764
msgid ""
"`CRS with local mutation "
"<http://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#controlled-"
"random-search-crs-with-local-mutation>`__ is part of the family of the "
"CRS optimizers. The CRS optimizers start with a random population of "
"points, and randomly evolve these points by heuristic rules. In the case "
"of CRS with local mutation, the evolution is a randomized version of the "
":ref:`Nelder-Mead` local optimizer."
msgstr ""

#: ../../aqua/optimizers.rst:772
msgid ""
"When referring to CRS with local mutation declaratively inside Aqua, its "
"code ``name``, by which Aqua dynamically discovers and loads it, is "
"``CRS``."
msgstr ""

#: ../../aqua/optimizers.rst:779
msgid "DIviding RECTangles algorithm - Locally based (DIRECT-L)"
msgstr ""

#: ../../aqua/optimizers.rst:781
msgid ""
"DIviding RECTangles (DIRECT) is a deterministic-search algorithms based "
"on systematic division of the search domain into increasingly smaller "
"hyperrectangles. The `DIRECT-L "
"<http://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#direct-and-"
"direct-l>`__ version is a variant of DIRECT that makes the algorithm more"
" biased towards local search, so that it is more efficient for functions "
"with few local minima."
msgstr ""

#: ../../aqua/optimizers.rst:789
msgid ""
"When referring to DIRECT-L declaratively inside Aqua, its code ``name``, "
"by which Aqua dynamically discovers and loads it, is ``DIRECT_L``."
msgstr ""

#: ../../aqua/optimizers.rst:796
msgid "DIviding RECTangles algorithm - Locally based - RANDomized (DIRECT-L-RAND)"
msgstr ""

#: ../../aqua/optimizers.rst:798
msgid ""
"`DIRECT-L-RAND <http://nlopt.readthedocs.io/en/latest/NLopt_Algorithms"
"/#direct-and-direct-l>`__ is a variant of :ref:`DIviding RECTangles "
"algorithm - Locally based (DIRECT-L)` that uses some randomization to "
"help decide which dimension to halve next in the case of near-ties."
msgstr ""

#: ../../aqua/optimizers.rst:804
msgid ""
"When referring to DIRECT-L-RAND declaratively inside Aqua, its code "
"``name``, by which Aqua dynamically discovers and loads it, is "
"``DIRECT_L_RAND``."
msgstr ""

#: ../../aqua/optimizers.rst:811
msgid "Evolutionary Strategy algorithm with CaucHy distribution (ESCH)"
msgstr ""

#: ../../aqua/optimizers.rst:813
msgid ""
"`ESCH <http://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#esch-"
"evolutionary-algorithm>`__ is an evolutionary algorithm for global "
"optimization that supports bound constraints only. Specifically, it does "
"not support nonlinear constraints."
msgstr ""

#: ../../aqua/optimizers.rst:819
msgid ""
"When referring to ESCH declaratively inside Aqua, its code ``name``, by "
"which Aqua dynamically discovers and loads it, is ``ESCH``."
msgstr ""

#: ../../aqua/optimizers.rst:826
msgid "Improved Stochastic Ranking Evolution Strategy (ISRES)"
msgstr ""

#: ../../aqua/optimizers.rst:828
msgid ""
"`ISRES <http://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#isres-"
"improved-stochastic-ranking-evolution-strategy>`__ is an algorithm for "
"nonlinearly-constrained global optimization. It has heuristics to escape "
"local optima, even though convergence to a global optima is not "
"guaranteed. The evolution strategy is based on a combination of a "
"mutation rule and differential variation. The fitness ranking is simply "
"via the objective function for problems without nonlinear constraints. "
"When nonlinear constraints are included, the `stochastic ranking proposed"
" by Runarsson and Yao "
"<https://notendur.hi.is/^tpr/software/sres/Tec311r.pdf>`__ is employed. "
"This method supports arbitrary nonlinear inequality and equality "
"constraints, in addition to the bound constraints."
msgstr ""

#: ../../aqua/optimizers.rst:839
msgid ""
"When referring to ISRES declaratively inside Aqua, its code ``name``, by "
"which Aqua dynamically discovers and loads it, is ``ISRES``."
msgstr ""

